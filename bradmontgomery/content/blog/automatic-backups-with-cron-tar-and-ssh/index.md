---
date: '2007-01-10T21:10:00+00:00'
title: Automatic Backups with cron, tar, and SSH
draft: false
tags:
- Backup
- Linux
- OS
- X
slug: automatic-backups-with-cron-tar-and-ssh
description: Everyone knows that ...
markup: html
url: /blog/automatic-backups-with-cron-tar-and-ssh/
aliases:
- /blog/2007/01/10/automatic-backups-with-cron-tar-and-ssh/

---

Everyone knows that backups are important, but how many of us regularly back up our own websites, blogs, or whatever? Well, I've put together a relatively simple way for my Mac to log into my Linux-based webserver, archive some content, and download it for me.  All of this is done Automatically, too!<br /><br />The first thing I had to do, was set up my home machine (a Mac, but any Unix-based system should work, too) so that it could log into a remote host without requiring me to type a password.  This is accomplished by generating a public key, which I can store on my webserver.  To do this, I opened a Terminal, and typed the following:<pre>ssh-keygen -t rsa</pre> This generated the following output, and I accepted all default values... even the empty passphrase.  This is important, because I don't want to type anything to log into my server!<pre><br />Generating public/private rsa key pair.<br />Enter file in which to save the key (/home/brad/.ssh/id_rsa):<br />Enter passphrase (empty for no passphrase): <br />Enter same passphrase again: <br />Your identification has been saved in /home/brad/.ssh/id_rsa.<br />Your public key has been saved in /home/brad/.ssh/id_rsa.pub.</pre><br /><br />Now, I need to store the public key (id_rsa.pub) on my webserver. Again, from my Terminal (in my home directory), I issue the following command to log into my webserver (Note that you'd need to change the following command to fit your needs).<br /><pre>cat .ssh/id_rsa.pub | ssh myusername@mywebserver.com 'cat >> .ssh/authorized_keys'</pre><br />I'll need to type my password here, but after executing this command, I should be able to ssh into my remote server without ever typing my password again!<br /><br />Now that I've accomplished that, I need to write some simple bash scripts that will archive my web content. <b>On my Webserver:</b> For simplicity sake, let's save this in our home directory in a file named www.sh.  Inside that file we put the following shell script, which uses tar to archive my web directory and then calls bzip2 to compress the tar file.  Upon execution, I should have a file with a name similar to 2007-01-10.tar.bz2. Notice that you could also modify this script to invoke mysql_dump or pg_dump if you needed to backup a database as well! <br /><pre><br />#!/bin/bash<br />DATE=`date +%F`  # Grab the date<br />TARFILE=$DATE-www.tar # Use it to create a filename<br />tar -cf $TARFILE public_html<br />bzip2 $TARFILE # compress the tarfile</pre><br /><br />Now that I have a script that will archive my content, I need to schedule a cron job to run it on a regular basis. I can use the following command to view my cron jobs<pre>crontab -l</pre>And, to edit my cron jobs, I just use:<pre>crontab -e</pre>This will open your system's default editor (vi for me), where we need to add the information that tells cron when to run our www.sh script. I added the following: <pre>30 1 * * * ~/www.sh</pre>This tells cron to run my backup script every morning at 1:30am (system time).  See <a href="http://en.wikipedia.org/wiki/Cron">wikipedia</a> for more info on cron.<br /><br />Almost done!  I've now got my webserver generating automatic nightly backups, but how can I transfer them to my local machine? Well, we'll make use of our newly generated public key to do this!  First I need to write another simple bash script that will use scp to copy the backup that I generated on the server.<b>On my local machine</b>, I create a file called transfer.sh, which contains the following:<br /><pre>#!/bin/bash<br />DATE=`date +%F` # Grab the date<br /># I'm downloading file with this name<br />FILE=$DATE-www.tar.bz2 <br /><br /># put the file on my local Desktop<br />scp myusername@mywebserver.com:~/$FILE ~/Desktop<br /><br /># delete the file from the server<br />ssh myusername@mywebserver.com rm -f ~/$FILE<br /></pre><br />Now all I have to do is schedule a cron job on my local machine, and I've got an automatic website backup! Since I'm using Mac OS X, this works just like it does on a Linux box.  In my terminal I type the following command:<br /><pre>crontab -e</pre><br />This opens vi for me, so I type the following, which will exectute my transfer.sh script every morning at 7:30am.<br /><br /><pre><br />30 7 * * * $HOME/transfer.sh<br /></pre><br /><br />That's It! One side note, however... the bash scripts that I've written must be executable by the user under which cron runs.  Usually, the following command will  make a file executable for its owner:<br /><pre>chmod u+x filename</pre><br />Resources:<br /><a href="http://www.debian-administration.org/articles/152">debian-administration.org</a><br /><a href="http://en.wikipedia.org/wiki/Cron">Wikipedia</a><br /><a href="http://www.linuxproblem.org/art_9.html">linuxproblem.org</a><div class="blogger-post-footer"><img width='1' height='1' src='https://blogger.googleusercontent.com/tracker/4123748873183487963-5549773385995764114?l=bradmontgomery.blogspot.com' alt='' /></div>